{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import _BaseDiscreteNB, MultinomialNB\n",
    "# import modules/downloads\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "extra_paragraphs = pd.read_csv(\"./tidy_text_data.csv\")\n",
    "extra_paragraphs = extra_paragraphs[:1250]\n",
    "\n",
    "train = pd.read_csv(\"./train.txt\", sep=\";\")\n",
    "train.columns = [\"text\",\"sentiment\"]\n",
    "test = pd.read_csv(\"./test.txt\", sep=\";\")\n",
    "test.columns = [\"text\",\"sentiment\"]\n",
    "\n",
    "train_text = train.text\n",
    "extra_paragraphs_text = extra_paragraphs.text_new\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000, min_df=3, binary=True, \n",
    "                             stop_words=\"english\")\n",
    "X_supervised = vectorizer.fit_transform(train_text)\n",
    "X_unsupervised = vectorizer.transform(extra_paragraphs_text)\n",
    "\n",
    "X_supervised.shape, X_unsupervised.shape\n",
    "\n",
    "X_test = vectorizer.transform(test.text)\n",
    "test_sentiment = test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17249, 4885)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "combined_X = np.concatenate((X_supervised.toarray(), X_unsupervised.toarray()), axis = 0)\n",
    "combined_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = np.ones(X_supervised.shape[0], dtype=int)\n",
    "mask_2 = np.zeros(X_unsupervised.shape[0], dtype=int)\n",
    "\n",
    "mask = np.concatenate((mask_1,mask_2), axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# function to get unique values\n",
    "def unique(list1):\n",
    " \n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "     \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "\n",
    "topics = unique(train['sentiment'])\n",
    "y_train = np.array([topics.index(k) for k in train['sentiment']],dtype=int)\n",
    "y_test = np.array([topics.index(k) for k in test_sentiment],dtype=int)\n",
    "combined_Y = np.concatenate((y_train, np.zeros(X_unsupervised.shape[0], dtype=int)), axis=0)\n",
    "print(len(unique(combined_Y)))\n",
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import _BaseDiscreteNB, MultinomialNB\n",
    "\n",
    "class SoftMultinomialNB(MultinomialNB):\n",
    "\n",
    "    # Note y = numpy array of size n x k containing \n",
    "    # for each example n, the probability of it belonging to class k.\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):        \n",
    "      _, n_features = X.shape\n",
    "      self.n_features_in_ = n_features \n",
    "\n",
    "      Y = y\n",
    "      self.classes_ = np.arange(Y.shape[1])\n",
    "\n",
    "      if sample_weight is not None:\n",
    "          Y = Y.astype(np.float64, copy=False)\n",
    "          sample_weight = np.asarray(sample_weight)\n",
    "          sample_weight = np.atleast_2d(sample_weight)\n",
    "          Y *= check_array(sample_weight).T\n",
    "\n",
    "      class_prior = self.class_prior\n",
    "\n",
    "      # Count raw events from data before updating the class log prior\n",
    "      # and feature log probas\n",
    "      n_effective_classes = Y.shape[1]\n",
    "\n",
    "      self._init_counters(n_effective_classes, n_features)\n",
    "      self._count(X, Y)\n",
    "      alpha = self._check_alpha()\n",
    "      self._update_feature_log_prob(alpha)\n",
    "      self._update_class_log_prior(class_prior=class_prior)\n",
    "      return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semisupervised_EM(init_model,k, X, y_true, mask, max_iters=200, soft=False,\n",
    "                      return_model=False):\n",
    "    '''\n",
    "    Implements semi-supervised Hard or Soft EM model.\n",
    "        - init_model is a function for initializing the SoftMultinomialNB model\n",
    "            (hint: you are going to want to use this parameter later in the\n",
    "             homework to initialize in a different model)\n",
    "        - X is an (n x d) numpy array\n",
    "        - y_true - 0/1 class labels defined previously.\n",
    "        - mask - an array of size n containing True or False indicating if you\n",
    "            are to use the true labels of that example or not.\n",
    "        - soft - a boolean variable, which if true, performs soft EM and\n",
    "            otherwise performs hard EM\n",
    "        - return_model - specifies whether or not to return the NB classifier\n",
    "    \n",
    "    Return a list of size n, containing the class label (between 0 and k-1) of\n",
    "    each item.\n",
    "    '''\n",
    "    n, d = X.shape\n",
    "    X.shape\n",
    "    temp = np.zeros((len(y_true),k))\n",
    "    for i in range(temp.shape[0]):\n",
    "        temp[i,y_true[i]] = 1\n",
    "    \n",
    "    m = init_model()\n",
    "    x_train = X[mask]\n",
    "    # y_train = y_true[mask]\n",
    "    y_train_mat = temp[mask]\n",
    "    m.fit(x_train, y_train_mat)\n",
    "    y_hat = None\n",
    "    for _ in tqdm(range(max_iters)):\n",
    "        # E Step: predict the latent variables (y_hat) using the model\n",
    "        newy_hat = m.predict_proba(X)\n",
    "        if not soft:\n",
    "            newy_hat = np.floor(0.5 + newy_hat)\n",
    "        # converged, return model\n",
    "        if (newy_hat == y_hat).all():\n",
    "            if return_model:\n",
    "                return newy_hat, m\n",
    "            else: \n",
    "                return newy_hat\n",
    "        y_hat = newy_hat\n",
    "        #M Step: retrain the model using the latent variables\n",
    "        y_hat[mask] = y_train_mat\n",
    "        m.fit(X, y_hat)\n",
    "\n",
    "    y_hat = m.predict(X)\n",
    "    if return_model:\n",
    "        return y_hat, m\n",
    "    else:\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SoftMultinomialNB' object has no attribute 'super'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3c805e23edbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msemisupervised_EM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSoftMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-406b51530b10>\u001b[0m in \u001b[0;36msemisupervised_EM\u001b[1;34m(init_model, k, X, y_true, mask, max_iters, soft, return_model)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# y_train = y_true[mask]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0my_train_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-92e51d64b3da>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     12\u001b[0m       \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SoftMultinomialNB' object has no attribute 'super'"
     ]
    }
   ],
   "source": [
    "yhat, m = semisupervised_EM(lambda: SoftMultinomialNB(), 6, combined_X, combined_Y, mask, return_model=True, soft=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakdown_clusters_by_ground_truth(y_true, cluster_labels, display=True) :\n",
    "    \"\"\"\n",
    "    If display = True, \n",
    "        display counts of documents of each topic in each of the generated\n",
    "        clusters and plot the same.\n",
    "        \n",
    "    Returns v-measure of cluster labels with respect to ground truth\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(y_true) == len(cluster_labels), \"Length of Target and cluster labels don't match\"\n",
    "    df = pd.DataFrame({'topic' : y_true, 'cluster' : cluster_labels})\n",
    "    counts = df.groupby(['topic', 'cluster']).size().reset_index().rename(columns={0:'count'})\n",
    "    if display :\n",
    "        print(counts)\n",
    "        sns.barplot(x=\"topic\", y=\"count\", hue=\"cluster\", data=counts)\n",
    "    return v_measure_score(y_true, cluster_labels)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "prediction = m.predict(X_supervised)\n",
    "print(prediction)\n",
    "print(len(unique(prediction)))\n",
    "train_eval = lambda cluster_labels, d=True: breakdown_clusters_by_ground_truth(prediction, y_train, d)\n",
    "\n",
    "print(f'The f1 score of fully trained model = {f1_score(y_train, prediction,average=\"macro\")}')\n",
    "print(f'The v-measure of fully trained model = {train_eval(prediction, d=True)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
