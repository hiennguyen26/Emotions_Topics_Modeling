{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import modules and test files\n",
    "\n",
    "train_set = pd.read_csv(\"./tweets_data.csv\")\n",
    "train_set.head()\n",
    "train_set = train_set[:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function    \n",
    "def lemmatize(word_list, ptags):\n",
    "    '''Lemmatizes words based on allowed postags, input format is list of sublists \n",
    "       with strings'''\n",
    "    spC = spacy.load('en_core_web_sm')\n",
    "    lem_lists =[]\n",
    "    for vec in word_list:\n",
    "        sentence = spC(\" \".join(vec))\n",
    "        lem_lists.append([token.lemma_ for token in sentence if token.pos_ in ptags])\n",
    "    \n",
    "    return lem_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt requirements\n",
    "# pandas\n",
    "# numpy\n",
    "# spacy>=2.2.4\n",
    "# nltk>=3.4.5\n",
    "# gensim>=3.8.3\n",
    "# plotnine>=0.6.0\n",
    "# tomotopy>=0.7.1\n",
    "# wordcloud>=1.7.0\n",
    "\n",
    "import numpy as np \n",
    "import spacy\n",
    "import nltk as nltk\n",
    "import gensim\n",
    "import plotnine\n",
    "import tomotopy\n",
    "import re\n",
    "# import wordcloud\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "st_words = stopwords.words('english')\n",
    "extra_stops=['from','subject','re', 'edu','use']\n",
    "st_words.extend(extra_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upset', 'update', 'facebook', 'texting', 'cry', 'result', 'school']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = train_set.text.values.tolist()\n",
    "word_list = [simple_preprocess(txt, deacc=True, min_len=3) for txt in doc_list]\n",
    "bigram = Phrases(word_list, min_count=5, threshold=100) # use original wordlist to build model\n",
    "bigram_model = Phraser(bigram)\n",
    "word_list_nostops = [[word for word in txt if word not in st_words] for txt in word_list]\n",
    "word_bigrams = [bigram_model[w_vec] for w_vec in word_list_nostops]\n",
    "word_list_lemmatized = lemmatize(word_bigrams, ptags=['NOUN','VERB','ADV','ADJ'])\n",
    "word_list_lemmatized[0][:7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "term_weight = tp.TermWeight.ONE\n",
    "hdp = tp.HDPModel(tw=term_weight, min_cf=5, rm_top=7, gamma=1,\n",
    "                  alpha=0.1, initial_k=10, seed=99999)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vec in word_list_lemmatized:\n",
    "#     hdp.add_doc(vec)\n",
    "\n",
    "# # Initiate sampling burn-in  (i.e. discard N first iterations)\n",
    "# hdp.burn_in = 100\n",
    "# hdp.train(0)\n",
    "# print('Num docs:', len(hdp.docs), ', Vocab size:', hdp.num_vocabs,\n",
    "#       ', Num words:', hdp.num_words)\n",
    "# print('Removed top words:', hdp.removed_top_words)\n",
    "\n",
    "# # Train model\n",
    "# for i in range(0, 1000, 100):\n",
    "#     hdp.train(100) # 100 iterations at a time\n",
    "#     print('Iteration: {}\\tLog-likelihood: {}\\tNum. of topics: {}'.format(i, hdp.ll_per_word, hdp.live_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import sys\n",
    "\n",
    "def train_HDPmodel(hdp, word_list, mcmc_iter, burn_in=100, quiet=False):\n",
    "    '''Wrapper function to train tomotopy HDP Model object\n",
    "    \n",
    "    *** Inputs**\n",
    "    hdp: obj -> initialized HDPModel model\n",
    "    word_list: list -> lemmatized word list of lists\n",
    "    mcmc_iter : int -> number of iterations to train the model\n",
    "    burn_in: int -> MC burn in iterations\n",
    "    quiet: bool -> flag whether to print iteration LL and Topics, if True nothing prints out\n",
    "    \n",
    "    ** Returns**\n",
    "    hdp: trained HDP Model \n",
    "    '''\n",
    "    \n",
    "    # Add docs to train\n",
    "    for vec in word_list:\n",
    "        print(vec)\n",
    "        hdp.add_doc(vec)\n",
    "\n",
    "    # Initiate MCMC burn-in \n",
    "    hdp.burn_in = 100\n",
    "    hdp.train(0)\n",
    "    print('Num docs:', len(hdp.docs), ', Vocab size:', hdp.num_vocabs, ', Num words:', hdp.num_words)\n",
    "    print('Removed top words:', hdp.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "\n",
    "    # Train model\n",
    "    step=round(mcmc_iter*0.10)\n",
    "    for i in range(0, mcmc_iter, step):\n",
    "        hdp.train(step, workers=3)\n",
    "        if not quiet:\n",
    "            print('Iteration: {}\\tLog-likelihood: {}\\tNum. of topics: {}'.format(i, hdp.ll_per_word, hdp.live_k))\n",
    "        \n",
    "    print(\"Done\\n\")  \n",
    "    \n",
    "    return hdp\n",
    "    \n",
    "        \n",
    "def get_hdp_topics(hdp, top_n=10):\n",
    "    '''Wrapper function to extract topics from trained tomotopy HDP model \n",
    "    \n",
    "    ** Inputs **\n",
    "    hdp:obj -> HDPModel trained model\n",
    "    top_n: int -> top n words in topic based on frequencies\n",
    "    \n",
    "    ** Returns **\n",
    "    topics: dict -> per topic, an arrays with top words and associated frequencies \n",
    "    '''\n",
    "    \n",
    "    # Get most important topics by # of times they were assigned (i.e. counts)\n",
    "    sorted_topics = [k for k, v in sorted(enumerate(hdp.get_count_by_topics()), key=lambda x:x[1], reverse=True)]\n",
    "\n",
    "    topics=dict()\n",
    "    \n",
    "    # For topics found, extract only those that are still assigned\n",
    "    for k in sorted_topics:\n",
    "        if not hdp.is_live_topic(k): continue # remove un-assigned topics at the end (i.e. not alive)\n",
    "        topic_wp =[]\n",
    "        for word, prob in hdp.get_topic_words(k, top_n=top_n):\n",
    "            topic_wp.append((word, prob))\n",
    "\n",
    "        topics[k] = topic_wp # store topic word/frequency array\n",
    "        \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model one\n",
      "['lean', 'empty', 'guard', 'desk', 'hear', 'footstep', 'straighten', 'shove', 'hand', 'uniform', 'figure', 'emerge', 'door', 'lead', 'basement']\n",
      "['nightstand', 'bed', 'analogue', 'phone', 'brochure', 'take', 'front', 'desk', 'advertise', 'free', 'steak', 'dinner', 'lounge']\n",
      "['happy', 'beam', 'hardwood', 'car', 'job', 'girlfriend', 'life', 'good', 'get', 'well', 'imminent', 'release', 'societal', 'obligation', 'inconvenience', 'full', 'time', 'job', 'none', 'benefit']\n",
      "['cave', 'deep', 'green', 'water', 'search', 'xibalba', 'underworld', 'alone', 'save', 'snake', 'cave', 'cricket', 'put', 'miner', 'lamp', 'begin', 'explore', 'stumble', 'swallow', 'large', 'gulp', 'water', 'spit', 'muddy', 'gritty', 'aftertaste', 'recover', 'ease', 'pair', 'flipper', 'move', 'far', 'cave', 'meander', 'tunnel']\n",
      "['sophie', 'forget', 'completely', 'return', 'eventually', 'enough', 'surprise', 'easy', 'forget', 'body', 'ignore', 'wound', 'precise', 'moment', 'helper', 'mind', 'drape', 'sheet', 'pain', 'let', 'see', 'tonight', 'go', 'anywhere', 'today', 'way', 'never', 'let', 'rest', 'let', 'lain', 'night', 'lay', 'bed', 'wait', 'sleep', 'final', 'scene', 'play']\n",
      "['nameless', 'woman', 'unicorn', 'skin', 'bear', 'purple', 'pink', 'scar', 'ashen', 'sky', 'eye', 'spectral', 'grit', 'oil', 'yesterday', 'brutality', 'pain', 'boil', 'real', 'life', 'exhaust', 'kingdom', 'wait', 'seat', 'throne', 'mind', 'moment', 'giant', 'orb', 'expand', 'contract', 'exultingly', 'diminish', 'body', 'blind', 'white', 'cruel', 'descend', 'compassion']\n",
      "['city', 'planner', 'designer', 'want', 'name', 'utopia', 'soon', 'realize', 'project', 'consider', 'name', 'glory', 'truly', 'glorious', 'bear', 'perfect', 'city', 'perfect', 'building', 'perfect', 'people', 'virtually', 'simply', 'perfect', 'problem', 'perfect', 'bear', 'deformity', 'seem', 'baffle', 'modern', 'medicine', 'try', 'medical', 'community', 'able', 'willing', 'explain', 'nub', 'leg', 'therefore', 'nin', 'bear', 'quickly', 'remove', 'infant', 'delivery', 'room', 'city', 'official', 'rush', 'explain', 'sorry', 'city', 'city', 'accept', 'responsibility', 'less', 'perfect', 'child', 'grieve', 'couple', 'move', 'head', 'birth', 'queue', 'life', 'quickly', 'return', 'glorious', 'norm', 'long', 'couple', 'produce', 'child', 'perfect', 'way']\n",
      "['sit', 'outside', 'talk', 'client', 'even', 'mean', 'even', 'focus', 'video', 'game', 'patio', 'laugh', 'stupid', 'joke', 'unsure', 'past', 'week', 'long', 'stay', 'place', 'hate', 'home', 'nanny', 'many', 'room']\n",
      "['final', 'trip', 'draken', 'salvage', 'team']\n",
      "['awake', 'little', 'bedroom', 'upper', 'floor', 'townhouse', 'apartment', 'pull', 'cover', 'roll', 'spot', 'pee', 'little', 'boy', 'center']\n",
      "['cabin', 'boy', 'heimun', 'vhi', 'ladder', 'eden', 'head', 'main', 'crosswalk', 'chief', 'engineering', 'officer', 'usual', 'post', 'captain', 'threaten', 'skin', 'entire', 'prime', 'crew', 'gather', 'belly', 'immediately', 'minute', 'ago']\n",
      "['floor', 'apartment', 'sit', 'top', 'long', 'twisting', 'staircase', 'elevator', 'fine', 'city', 'power', 'go', 'weeklong', 'smack', 'binge', 'image', 'panic', 'rioting', 'emergency', 'warning', 'eventually', 'static', 'channel', 'dream', 'nod', 'sure']\n",
      "['poor', 'george', 'trumbull', 'dead', 'day', 'know']\n",
      "['email', 'receive', 'error', 'delete', 'immediately', 'open', 'jill', 'stare', 'monitor', 'wonder', 'email', 'require', 'first', 'day', 'new', 'job', 'help', 'find', 'missing', 'friend', 'sit', 'chair', 'week', 'ago']\n",
      "['glisten', 'sidewalk', 'fandango', 'pink', 'malleable', 'fresh', 'plasticity', 'film', 'frothy', 'saliva', 'comfortably', 'settle', 'crevice', 'tiny', 'bubble', 'tickle', 'elastic', 'flesh', 'pop', 'frightful', 'sun', 'dread', 'thing', 'end', 'reduce', 'insipid', 'blotch', 'otherwise', 'clear', 'pavement']\n",
      "['story', 'daughter', 'gold', 'rich', 'king', 'create', 'kingdom', 'apple', 'hungry', 'poor', 'cry', 'sleep', 'daughter', 'gold', 'rich', 'king', 'step', 'public', 'road', 'leave', 'golden', 'first', 'time', 'golden', 'bracelet', 'hang', 'arm', 'cover', 'wrist', 'elbow', 'golden', 'ring', 'cover', 'finger', 'toe', 'golden', 'anklet', 'wind', 'leg', 'ankle', 'knee', 'compel', 'dream', 'leave', 'moonless', 'night']\n",
      "['hand', 'nickel', 'young', 'lady', 'catrine', 'say', 'eyebrow', 'vex', 'crow']\n",
      "['trip', 'grandpa', 'organize', 'dance', 'recital', 'go', 'wrong', 'way', 'big', 'shame', 'stare', 'miss', 'first', 'step', 'many', 'laugh', 'inevitable', 'fall', 'come', 'even', 'parent', 'particularly', 'remember', 'dad', 'hold', 'daughter', 'firm', 'embrace', 'flashed', 'fake', 'smile', 'say', 'perhaps', 'well', 'choose', 'sport', 'chess', 'dart', 'example']\n",
      "['shore', 'swamp', 'teenager', 'fall', 'love', 'twilight', 'autumn', 'night', 'kick', 'shoe', 'roll', 'pant', 'slide', 'sandal', 'hold', 'dress', 'right', 'hand', 'grimy', 'jar', 'step', 'house', 'unscrewed', 'lid', 'hand', 'let', 'dress', 'wade', 'water', 'together', 'dash', 'shallow', 'catch', 'odd', 'dozen', 'firefly', 'release', 'catch', 'brush', 'stray', 'hair', 'ear', 'kiss', 'first', 'time']\n",
      "['hardenbrook', 'peered', 'front', 'window', 'coffee', 'shop']\n",
      "['city', 'go', 'sleep', 'city', 'leave', 'world', 'still', 'sleep', 'frequent', 'nevertheless', 'city', 'sleep', 'ever', 'thus', 'story', 'begin', 'sky', 'thick', 'layer', 'smog', 'even', 'thick', 'layer', 'fog', 'slightly', 'less', 'thick', 'nonetheless', 'significant', 'layer', 'cloud', 'rain', 'plummet', 'black', 'plethora', 'star', 'various', 'arrangement', 'say', 'see', 'spoon', 'bear', 'even', 'warrior', 'star', 'truly', 'utensil', 'animal', 'people', 'occupation', 'sky', 'people', 'like', 'add', 'mean', 'random', 'scattering', 'let', 'name', 'star', 'shape', 'enjoy', 'constellation', 'see', 'often', 'city', 'dweller']\n",
      "['diabete', 'ever', 'little', 'boy', 'parent', 'well', 'deal', 'difficult', 'situation', 'spite', 'problem', 'turn', 'good', 'look', 'fellow', 'little', 'frail', 'head', 'full', 'dark', 'curly', 'hair', 'mean', 'ruggedly', 'handsome', 'footer', 'broad', 'shoulder', 'girl', 'call', 'cute', 'hear', 'hot', 'babe', 'college', 'call', 'cute', 'devastate', 'assume', 'chance', 'beautiful', 'damsel', 'book', 'cute', 'apply', 'baby', 'puppy', 'kitty', 'cat', 'debonair', 'dash', 'guy']\n",
      "['puck', 'come', 'run', 'back', 'metal', 'pipe', 'firmly', 'clench', 'watch', 'approach', 'profound', 'weariness', 'think', 'bring', 'throw', 'pipe', 'twentieth', 'time']\n",
      "['sure', 'absolutely', 'sure', 'really', 'hurt', 'even', 'scare']\n",
      "['young', 'lone', 'bright', 'blue', 'eye', 'name', 'often', 'lay', 'star', 'think', 'big', 'mysterious', 'world', 'longing', 'explore', 'however', 'become', 'mixed', 'evil', 'influence', 'move', 'ensure', 'time', 'think', 'escape', 'lurk', 'nearby', 'distance', 'hold']\n",
      "['run', 'ahead', 'parent', 'completely', 'immerse', 'information', 'board', 'time', 'information', 'board', 'want', 'real', 'thing']\n",
      "['get', 'say']\n",
      "['freddie', 'fern', 'old', 'couple', 'old', 'couple', 'truth', 'tell', 'matter', 'age', 'truth', 'seldom', 'surface', 'kid', 'grow', 'go', 'family', 'live', 'different', 'city', 'even', 'ask', 'parent', 'sell', 'house', 'buy', 'small', 'place', 'live', 'fern', 'ache', 'pain', 'independent', 'couple', 'like', 'privacy', 'see', 'grandchild', 'nice', 'live', 'close', 'enough', 'babysit', 'quite', 'matter']\n",
      "['apology']\n",
      "['police', 'detective', 'jake', 'see', 'horrific', 'injury', 'fatality', 'look', 'warped', 'ever']\n",
      "['time', 'almost', 'believe', 'real']\n",
      "['complete', 'engineering', 'degree', 'many', 'peer', 'deepak', 'prepare', 'study', 'parent', 'let', 'deepak', 'lure', 'dangerous', 'liaison', 'beef', 'eat', 'christian', 'girl', 'begin', 'search', 'suitable', 'girl', 'hyderabad', 'deepak', 'dark', 'short', 'puny', 'many', 'prospective', 'bride', 'impress', 'become', 'crystal', 'clear', 'modern', 'girl', 'keen', 'deepak', 'much', 'wide', 'net', 'cast', 'suitable', 'girl', 'locate', 'small', 'town', 'coastal', 'proper', 'eye', 'bushy', 'eyebrow', 'fine', 'hair', 'upper', 'lip', 'obviously', 'position', 'reject', 'deepak']\n",
      "['bleak', 'morning', 'sun', 'seep', 'cloud', 'cast', 'dim', 'light', 'thousand', 'grave', 'mark', 'home', 'avoid', 'living', 'scare', 'away', 'frightening', 'tale', 'legend']\n",
      "['turn']\n",
      "['bear', 'small', 'town', 'people', 'company', 'town', 'run', 'length', 'creek', 'hug', 'huge', 'mountain', 'mile', 'entire', 'town', 'side', 'creek', 'street', 'run', 'perpendicular', 'creek', 'bureaucracy', 'law', 'firm', 'corporate', 'office', 'know', 'live']\n",
      "['little_girl', 'sit', 'stoop', 'wait', 'wonder']\n",
      "['pietro', 'scratch', 'scar', 'pickle', 'leg', 'souvenir', 'african', 'insect', 'treatment', 'available', 'infected', 'bite', 'wound', 'scrape', 'flesh', 'blood', 'flow', 'apply', 'iodine', 'pain', 'open', 'old', 'wound', 'still', 'fresh', 'memory']\n",
      "['soon', 'afternoon', 'nap', 'boy', 'take', 'bike', 'slice', 'subdivision', 'make', 'way', 'narrow', 'street', 'old', 'neighborhood', 'mother', 'grow', 'father', 'grow', 'farm']\n",
      "['sit', 'prickly', 'green', 'carpet', 'try', 'force', 'focus', 'detach', 'decision', 'steve', 'rigid', 'body', 'yesterday', 'impartial', 'bystander', 'wait', 'patiently', 'side', 'decision', 'make']\n",
      "['step', 'desert', 'winter', 'last', 'day', 'vacation', 'appear', 'dead']\n",
      "['snoop', 'wife', 'closet', 'football', 'game', 'away', 'try', 'connect', 'family', 'tell', 'feel', 'reunion', 'find', 'tuck', 'shoebox', 'pair', 'sequine', 'high', 'heel']\n",
      "['shift', 'transition', 'depart', 'arrive']\n",
      "['bear', 'parent', 'live', 'small', 'brick', 'bungalow', 'low', 'middle', 'class', 'neighborhood', 'work', 'company', 'mother', 'stay', 'home', 'call', 'mother', 'follow', 'end', 'world', 'war']\n",
      "['parent', 'agree', 'weekend', 'tommy', 'sleep', 'friend', 'first', 'sleepover', 'last']\n",
      "['wake', 'weary', 'shoosh', 'airbrake', 'bus', 'jittere', 'engine', 'die', 'cold', 'glass', 'press', 'hard', 'lie', 'window']\n",
      "['grow', 'write', 'go', 'write', 'parent', 'write', 'breakup', 'write', 'feminism', 'write', 'people', 'ride', 'public', 'bus', 'write', 'divorce', 'write', 'dead', 'matter', 'grow', 'wealthy', 'stay', 'way', 'father', 'die', 'matter', 'considerable', 'medical', 'expense', 'bankrupt', 'family', 'father', 'botanist', 'love', 'old', 'movie', 'mother', 'love', 'swim', 'old', 'movie', 'husband']\n",
      "['memory', 'return', 'crawl', 'depth', 'blanket', 'darkness', 'envelop', 'take', 'place', 'newly', 'bear', 'laughter', 'place', 'smile', 'many', 'face', 'vague', 'nearly', 'forget', 'desire', 'achieve', 'safety', 'love', 'shelter', 'thing', 'grant', 'past', 'long', 'long', 'smile', 'memory', 'depth', 'blanket']\n",
      "['feel', 'melt']\n",
      "['summer', 'walnut', 'tree', 'dot', 'old', 'farmhouse', 'property', 'dead', 'skeletal', 'branch', 'reach', 'droop', 'nature', 'mournful', 'display', 'poison', 'land', 'worthwhile', 'grow']\n",
      "['nearby', 'one', 'bird', 'drop', 'stone', 'strike', 'hard', 'earth', 'swoop', 'front', 'door', 'fly', 'fast', 'black', 'blur', 'twilight', 'stick', 'beak', 'heavy', 'timber', 'door', 'dart', 'thwack', 'bird', 'lie', 'listen', 'eyelid', 'shut', 'tight', 'tremble', 'flinching', 'time', 'bird', 'hit', 'seem', 'hour', 'slid', 'bed', 'turn', 'light', 'avoid', 'wake', 'pregnant', 'wife', 'martinique']\n",
      "['life', 'long', 'gardener', 'know', 'freakish', 'summer', 'live', 'right', 'reach', 'headline', 'news', 'fume', 'quietly', 'lay', 'fresh', 'fruit', 'vegetable', 'flower', 'front', 'village', 'shop', 'do', 'year', 'quietly', 'horrify', 'lack', 'butterfly', 'come', 'think', 'bee', 'insect', 'creature', 'gone', 'friend', 'far', 'far', 'away', 'kind', 'subtle', 'massacre', 'go', 'conclude', 'brew', 'tea', 'day', 'start', 'insidious', 'really', 'damage', 'mostly', 'invisible', 'build', 'land', 'people', 'thing', 'airbrush', 'existence', 'gentle', 'breeze', 'cause', 'wheat', 'barley', 'crop', 'sway', 'gently', 'flat', 'norfolk', 'field', 'sheep', 'dog', 'play', 'early', 'drive', 'back', 'nearby', 'town', 'doze', 'peacefully', 'shop', 'front']\n",
      "['kill', 'day', 'anna', 'say', 'realize', 'let']\n",
      "['quill', 'grow', 'ratty', 'feather', 'twist', 'clump', 'sticky', 'ink', 'frank', 'remember', 'buy', 'run', 'finger', 'feather', 'gently', 'squeeze', 'catch', 'dip', 'number', 'time', 'scrape', 'side', 'wipe', 'excess', 'take', 'hat', 'head', 'put', 'drawer', 'remind', 'breathe', 'think', 'relax', 'try', 'smile', 'start', 'write']\n",
      "['addiction', 'terrible', 'thing', 'know', 'develop', 'bizarre', 'unhealthy', 'communion', 'wafer', 'dependency']\n",
      "['shadow', 'courthouse', 'clock', 'stand', 'new', 'think', 'head', 'aware', 'show', 'hold', 'attention', 'sit', 'cross', 'legged', 'back', 'memorial', 'year', 'old', 'shoulder', 'slowly', 'tan', 'ant', 'trap', 'magnify', 'glass', 'heating', 'far', 'quick', 'inquisitor']\n",
      "['go', 'tell', 'eventually', 'love', 'way']\n",
      "['wait', 'able', 'later', 'never', 'know', 'take', 'time', 'think', 'consequence', 'first', 'always', 'come', 'back', 'later']\n",
      "[]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Either `words` or `rawWords` must be filled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1982a5e16193>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtw_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mhdp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_HDPmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_list_lemmatized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcmc_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mhdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'models/hdp_model_'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtw_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\".bin\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-c7f8376c9ba3>\u001b[0m in \u001b[0;36mtrain_HDPmodel\u001b[1;34m(hdp, word_list, mcmc_iter, burn_in, quiet)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mhdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Initiate MCMC burn-in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Either `words` or `rawWords` must be filled."
     ]
    }
   ],
   "source": [
    "tw_list = [tp.TermWeight.ONE, # all terms weighted equally\n",
    "           tp.TermWeight.PMI, # Pointwise Mutual Information term weighting\n",
    "           tp.TermWeight.IDF] # down-weights high frequency terms, upweights low freq ones\n",
    "\n",
    "tw_names = ['one', 'pmi', 'idf']\n",
    "model_topics =[]\n",
    "\n",
    "for i, term_weight in enumerate(tw_list):\n",
    "    hdp = tp.HDPModel(tw=term_weight, min_cf=5, rm_top=7, gamma=1, alpha=0.1,\n",
    "                     initial_k=10, seed=99999)\n",
    "    \n",
    "    print(\"Model \" + tw_names[i] )\n",
    "    hdp = train_HDPmodel(hdp, word_list_lemmatized, mcmc_iter=1000)\n",
    "    hdp.save(''.join(['models/hdp_model_',tw_names[i],\".bin\"]))\n",
    "    \n",
    "    model_topics.append(get_hdp_topics(hdp, top_n=10))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
